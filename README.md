## wordspell

Поводом для реализации этого спелл-чекера послужила потребность в орфографической правке поисковых запросов.
Причем работать ему предстояло в реальном времени в высоконагруженном сервисе. Критичные параметры - скорость и ресурсоемкость.

Первоначально был использован `hunspell`. Нам удалось настроить кеширование таким образом, что его производительности хватало для реализации нашей задачи.
Но вот ресурсоемкость... Мы использовали нативную библиотеку `hunspell`, подключенную через `cgo`, и через нее нещадно [утекала память](https://habr.com/ru/companies/ncloudtech/articles/675390/).

Получалось, нам желательно было бы использовать библиотеку проверки орфографии, реализованную на чистом go. 
Поискав некоторое время готовые решения в сети, я остановился вот на таком [варианте (1)](https://habr.com/ru/companies/sbermegamarket/articles/673614/). 
Вот только хранить целиком словарь удалений и подгружать его в память представлялось достаточно накладным. 

Мне понравилась идея с использованием bloom-фильтра, изложенная вот [здесь (2)](https://habr.com/ru/articles/346618/). 
Хотя реализовывать весь алгорим, описанный в статье по ссылке [(2)](https://habr.com/ru/articles/346618/), на мой взгляд, нет смысла - 
в статье по ссылке [(1)](https://habr.com/ru/companies/sbermegamarket/articles/673614/) достаточно убедительно показано, что в нашем случае не нужно учитывать контекст. 

Вот так в результате возникла идея гибридной реализации - мы берем за основу алгоритм [SymSpell](https://wolfgarbe.medium.com/1000x-faster-spelling-correction-algorithm-2012-8701fcd87a5f), 
но не храним индекс удалений. Все удаления, построенные по основному индексу, мы храним в bloom-фильтре, и если слово не нашлось в базовом индексе, 
мы строим по нему набор удалений до глубины в 2 символа, отсекаем все, для чего bloom-фильтр отрицателен (а он не дает ложно-отрицательных результатов),
строим наборы вставок по 1-2 символам, и ищем то, что получилось, в базовом индексе. Если получаем несколько результатов, отбираем тот, 
у которого в индексе выше значение частоты встречаемости.

В результате получился спеллер в сотню раз быстрее `hunspell`, 
потребляющий 140-160 мегабайт оперативной памяти, и не создающий утечек - `wordspell`.

### Настройка и применение

Структура настроек выглядит вот так:
```
type Options struct {
	DataDir string
	SiteDB  postgres.Options
	Langs   []string
}

type postgres.Options struct {
	Host   string
	Port   int
	DBName string
	User   string
	Pass   string
	Test   bool
}
```
При этом поле `Langs` на данный момент избыточно - там по умолчанию используются два языка - `ru` и `en`.
Это поле предусмотрено на будущее, на данный момент работа корректора опирается на автоматическое распознавание
языка по одному слову. А это распознавание реализовано для трех "языков" - русского, английского и "численного".

Существуют библиотеки, которые довольно достоверно позволяют распознавать больше языков, но здесь они пока не используются.

Так что на данный момент достаточно указать значение `DataDir`. В этой директории должны присутствовать три файла:
```
ru.txt
en.txt
bloom.dat
```

В директории `rawdata` приведен инструмент для построения этих файлов и описание работы с ним.
Там же - текстовые примеры из Leipzig Corpora Collection
Они достаточно представительны, чтобы как минимум протестировать этот спеллер. Если точность коррекции с этими файлами достаточна,
можно их использовать и на проде.

Поле `SiteDB` - настройки для подключения к БД `postgres`, из которой извлекаются данные о торговых марках. Они обрабатываются особым образом.

Если `opt.SiteDB.Test` == `true`, в качестве индекса торговых марок будет использован индекс, построенный по файлу `testdata/trademark.txt`.
Такой результат достигается подключением к построителю индекса `/components/trademark/component.go` тестового источника имен торговых марок, имплементирующего
интерфейс `tradeMarkNamer`, читающего данные из тестового файла. Этот прием можно использовать для подключения адаптера, читающего данные откуда угодно.

Если `opt.SiteDB.Host` пуст, индекс торговых марок будет равен `nil`, и обработка имен торговых марок производиться не будет.  

#### Подключаем, используем:

```go
package main

import (
	"gitlab.sima-land.ru/dev-dep/dev/packages/go-wordspell"
	"gitlab.sima-land.ru/dev-dep/dev/packages/go-wordspell/options"
	"github.com/sirupsen/logrus"
)

func main() {
	logger := logrus.NewEntry(logrus.New())
	
	opt := &options.Options{
		DataDir: "<path to data dir>",
	}
	
	spell := wordspell.New(opt, logger)
	
	mistake := "фрУза для ИспраГления прЕ ключениР International Panasonic ищщо што-то"
	corrected := spell.Correct(mistake)
	
	logger.Infof("%s -> %s", mistake, corrected)
}
```

Тут на выходе получится "фраза для исправления приключение International Panasonic еще что-то"

* Сначала производится поиск торговых марок (возможно, многословных), и они включаются в исправленную фразу "как есть" точно на тех же местах, где и были. Язык для них не определяется.
* Потом проверяются попарно слитые вместе соседние слова. Если они не входят в торговые марки и относятся к одному языку, и этот язык - не `NumLangCode`. Так было исправлено словосочетание "прЕ ключениР" -> "приключение". Здесь работает основной алгоритм, проверка производится после приведения слова к нижнему регистру. Если замена была произведена, результат защищается от дальнейшей обработки и размещается в той же позиции изначального запроса вместо слов, из которых оно было получено.
* Потом производится проверка оставшихся слов, по следующим правилам: `correctedWord` будет равно `mistakeWord` в трех случаях:
  * `mistakeWord` - корректное число, возможно, дробное, возможно, с запятой в качестве десятичного символа.
  * `mistakeWord` есть в индексе и, следовательно, не требует правки
  * `wordspell` не может найти вариант исправления для `mistakeWord` - например, потому, что не может определить язык, или в силу бедности индексов. Если язык определить удалось, это значит, что в индексе нет ничего, что может быть получено из `mistake` добавлением/удалением до двух символов включительно, или заменой любых - максимум двух - символов - на любые другие символы алфавита в любой комбинации.

Русский алфавит включает все буквы плюс дефис, английский - еще плюс `backtick` и одинарную кавычку.

### Как `wordspell` определяет язык и для чего это нужно?

* Если слово содержит только цифры и (возможно) одну точку или запятую, это число. Спеллер ничего не делает с таким словом - просто возвращает "как есть".
* Для русского языка в слове имеют право находиться русские буквы и дефис.
* Для английского языка - английские буквы, дефис, `backtick` и одинарная кавычка.

Если слово - не число, мы подсчитываем в нем количество валидных и невалидных символов. 
Если валидных символов больше, а невалидных - не более 2, то мы относим слово к языку. 
Так что, в принципе, мы можем передать в `wordspell` слово с двумя пробелами, 
и если в нем окажется больше двух русских букв и не окажется ничего больше, 
это будет русское слово. Если в слове два невалидных символа, в каком-то из удалений не останется ни одного, 
и исправление для такого слова может быть найдено.

`wordspell` принимает поисковый запрос целиком, и сам очищает его от лишних символов и разбивает на слова. 
* Торговые марки ищутся по полному соответствию, включая регистр ("Sony" не равно "sony") и без учета языка (компания "Пупкин Ltd." будет найдена, если только присутствует в индексе торговых марок, и оставлена без изменения точно там же, где и была в изначальном запросе).
* Все остальные слова будет приведены к нижнему регистру и исправлены в соответствие с основным алгоритмом, с учетом языка.

### Выбор лучшего исправления

Построение набора всевозможных вставок в удаления - процесс дорогой. 
Поэтому мы возвращаем результат по первому из удалений, для которого его удается найти. 
Но если в рамках одного удаления нашлось несколько возможных исправлений, мы их ранжируем.

* Сначала по количеству вставок - если удалось что-то найти в пределах единичной вставки, возвращаем лучшее по частоте значение, и двойных вставок уже не делаем.
* Потом по частотам - если в рамках удаления нашлось несколько вариантов, возвращаем то из них, с которым связана наибольшая частота.

Получается, мы проверяем не все возможные варианты, так что точность алгоритма, с одной стороны, ожидается несколько ниже возможной, но, поскольку мы проверяем по индексу не удаления, 
а оригинальные (только "восстановленные") слова, эта неточность будет частично скомпенсирована.

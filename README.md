## wordspell

### Принципиальное устройство

Поводом для реализации этого спелл-чекера послужила потребность в орфографической правке поисковых запросов.
Причем работать ему предстояло в реальном времени в высоконагруженном сервисе. Критичные параметры - скорость и ресурсоемкость.

Первоначально был использован `hunspell`. Нам удалось настроить кеширование таким образом, что его производительности хватало для реализации нашей задачи.
Но вот ресурсоемкость... Мы использовали нативную библиотеку `hunspell`, подключенную через `cgo`, и через нее нещадно [утекала память](https://habr.com/ru/companies/ncloudtech/articles/675390/).

Получалось, нам желательно было бы использовать библиотеку проверки орфографии, реализованную на чистом go. 
Поискав некоторое время готовые решения в сети, я остановился вот на таком [варианте (1)](https://habr.com/ru/companies/sbermegamarket/articles/673614/). 
Вот только хранить целиком словарь удалений и подгружать его в память представлялось достаточно накладным. 

Мне понравилась идея с использованием bloom-фильтра, изложенная вот [здесь (2)](https://habr.com/ru/articles/346618/). 
Хотя реализовывать весь алгорим, описанный в статье по [ссылке (2)](https://habr.com/ru/articles/346618/), на мой взгляд, нет смысла - 
в статье по [ссылке (1)](https://habr.com/ru/companies/sbermegamarket/articles/673614/) достаточно убедительно показано, что в нашем случае не нужно учитывать контекст. 

Вот так в результате возникла идея гибридной реализации - мы берем за основу алгоритм [SymSpell](https://wolfgarbe.medium.com/1000x-faster-spelling-correction-algorithm-2012-8701fcd87a5f), 
но не храним индекс удалений. Все удаления, построенные по основному индексу, мы храним в bloom-фильтре, и если слово не нашлось в базовом индексе, 
мы строим по нему набор удалений до глубины в 2 символа, отсекаем все, для чего bloom-фильтр отрицателен (а он не дает ложно-отрицательных результатов),
строим наборы вставок по 1-2 символам, и ищем то, что получилось, в базовом индексе. Если получаем несколько результатов, отбираем тот, 
у которого в индексе выше значение частоты встречаемости.

В результате получился спеллер в сотню раз быстрее `hunspell`, 
потребляющий до 300 мегабайт оперативной памяти, и не создающий утечек - `wordspell`.

### Расширенный алгоритм

Такая идея позволяет нам построить спелл-чекер, проверяющий отдельные слова. 
Именно в таком контексте мы и использовали `hunspell` - заменяли все небуквенные символы на пробелы, 
разбивали запрос на слова по пробелам и проверяли отдельные слова. 
Но это не гибко, и в некоторых случаях совершенно не достаточно. Сразу же приходят в голову следующие кейсы:
- слова, ошибочно написанные слитно, например `игрушкидля`
- слова, ошибочно написанные раздельно, например `дихло фос`
- трейд-марки, например, `Microsoft` может быть ошибочно заменен на `micro soft`
- единицы измерения и проценты, например, `80 - 90%` или `335кг` (= `80-90%` и `335 кг` соответственно)
- измерения, например, `200 х 300 * 400см` (= `200*300*400 см`)
- размеры бумаги, например, `a5` (= `A5`)
- мерность, например, `4д` (= `4D`)

При этом трейд-марки у нас регистрозависимы (`International` - это трейдмарка, а `international` - международный), а размеры бумаги - нет (`A5` = `а5` = `а 5`).
Поиск паттернов, соответствующих измерениям, следует производить по всему запросу, а не по отдельным словам - например, вот тут `200 х 300 * 400см` - пять слов.

Причем то, что уже было изменено, не должно уже подвергаться проверкам по основному алгоритму. 
А паттерны для поиска специальных выражений следует искать в тексте ДО приведения его к нижнему регистру 
и до удаления всех небуквенных символов.

В результате возникло вот такое решение:
- заменяем в исходном запросе все символы, которые не могут встретиться ни в обычном слове, ни в паттерне специального выражения, ни в трейдмарке, на пробелы
- разбиваем запрос в слайс строк по пробелам
- обрабатываем последовательно набором процессоров предобработки
> Процессор пред- или постобработки имплементрует единственный метод `Process([]string) []string`.
> Если пре-процессор нашел и обработал какие-то из элементов слайса, он их размещает в том же месте обработанного слайса, 
> но с префиксом @ и с # вместо пробелов. Это исключает их из обработки в строках и позволяет позже распарсить в элементы, 
> защищенные от изменений (типа `DigestReady`).
- конвертируем результат работы процессоров предобработки в объект типа `Digest` и передаем в собственно методы wordspell.
> Все методы игнорируют ранее обработанные слова. Перед обработкой слова приводятся к нижнему регистру.
> Сначала метод `wordPair` проверяет все последовательные пары ранее необработанных слов, таким образом решается проблема `дихло фос`.
> Слова, написанные ошибочно слитно, обрабатываются методом `splittedWord`, который проверяет на наличие в индексе исходных слов, в которые добавляется пробел во все позиции, кроме первой и последней.
> И, наконец, метод `correctWord` тщательно выверяет все оставшиеся слова, применяя удаления/вставки до двух символов в случайных позициях, отметая не найденные удаления поиском в bloom-фильтре.
> Но по сравнению с "чистым" wordspell, подлежащих полной обработке слов остается меньше, и в результате расширенный алгоритм работает заметно быстрее.
- и, наконец, обрабатываем результат набором процессоров постобработки
> Процессор постобработки имплементирует тот же интерфейс, что и процессор предобработки, но на выходе у такого процессора уже не должно быть никаких промежуточных данных,
> поскольку результат постобработки будет просто склеен через пробел и возвращен в качестве исправленного запроса. В данный момент постбработчик у нас только один,
> он удаляет последовательные повторы, в том числе и вот такого типа: 
> `текст-дуп дуп` и `дуп дуп-текст`. В обоих случаях останется только слово с дефисом.

### Реализация

Тип `wordspell.Service` с единственным публичным методом `Correct(string) string` выполняет всю работу по исправлению запросов. 

Метод принимает полный поисковый запрос. Сначала из него удаляются все лишние символы, и результат токенизируется 
в слайс слов методом `strings.Fields`, который заодно удаляет из запроса все повторяющиеся пробелы.

Затем запрос обрабатывается препроцессорами. По большей части они работают по регулярным выражениям, 
за исключением процессора трейдмарок.
> `components/trademarkindex.Service` - индекс торговых марок, организованный для поиска в запросе торговой марки максимальной присутствующей в индексе длины.
> Например, если в индексе есть ТМ `InTurnational` и `InTurnational Panasonic`, то `InTurnational tail` будет распарсено как `InTurnational tail` (первое слово распознано как ТМ и исключено из спеллинга).
> Эта проверка производится регистрозависимо - например, `Inturnational` распознано не будет, оно будет исправлено, и в результате станет `international`.
> Точно так же `InTurnational Panasonic tail` останется без изменений, но `Inturnational Panasonic tail` превратится в `international panasuanic tail`, если только в `index.Service`
> найдутся слова `international` и `panasuanic`. В индексе ТМ может не быть `InTurnational`, но может присутствовать `InTurnational Panasonic`.
> Тогда `InTurnational tail` превратится в `international tail`.

Все препроцессоры, а также единственный пост-процессор, собраны в папке `processors`.

Затем результат конвертируется в тип `domain.Digest`, в котором все уже исправленные элементы помечены 
и защищены от дальнейших исправлений, и передается во внутренние методы сервиса `wordspell`.
Они работают, опираясь на компоненты, собранные в папке `components`.
- `langdetect.Component` - определитель языка, он нужен для того, чтобы уменьшить размер алфавита, применяемого для построения вставок.
> Мы разделяем три языка - русский, английский и "численный". Для "численного" языка нам не нужен алфавит, мы просто проверяем, 
> является ли последовательность цифр, возможно, с запятыми или точками, корректным числом? Если да, мы ничего с ним не пытаемся делать.
> Разделение же русского и английского языков позволяет нам использовать разные алфавиты для вставок вместо объединенного (что было бы много дороже).
- `wordmutate.Component` - механизм выполнения удалений и вставок в зависимости от определенного языка.
- `index.Service` - хранит индексы для русского и английского языков.
> Языков всего два. И для нужд этого чекера вряд ли стоит увеличивать их количество. Что, на самом деле, возможно, и в дальнейшем я попробую реализовать поддержку бОльшего количество языков.
- `bloomfilter.Component` - хранит все возможные удаления из всех слов, содержащихся в индексе `index.Service`, во всех языках.
> Удаление - это слово, из которого произвольно удален символ или два. Их очень много, поэтому мы их храним в битовой маске, позволяющей убедиться в том, что какого-то из удалений
> в ней ТОЧНО НЕТ. Сначала мы удаляем из проверяемого слова один символ и проверяем, есть ли такое удаление в фильтре. Если нет - удаляем из каждого из удалений еще по одному символу, и проверяем их.
> Если удалось что-то найти, мы достраиваем удаление до полного слова, добавляя в него произвольные символы из алфавита, и уже результат ищем в самом индексе. Если нашлось несколько результатов
> при работе с одним удалением, мы выбираем из них тот, который чаще встречался в тексте, по которому мы построили индекс. И если хоть что-то для текущего удаления нашлось, мы прекращаем поиски
> и отдаем результат.
> Ошибочно положительные находки в bloom-фильтре стоят довольно дорого, поэтому я задаю достаточно низкую вероятность таких находок при формировании фильтра. Что делает его больше 
> и немного медленнее, но существенно ускоряет работу спеллера.

После того, как `wordspell` исправит все подлежащие исправлению слова, `domain.Digest` рендерится в слайс слов и передается в постобработку.
Постобработчик на данный момент только один, он удаляет повторы и отдает "почищенный" слайс, который джойнится в строку через пробел, 
и отдается в качестве исправленного запроса.

### Настройка и применение

Структура настроек выглядит вот так:
```
type Options struct {
	Bloom    bloomfilter.Options
	SiteDB   postgres.Options
	S3Client s3client.Options
	S3Data   s3repo.Options
	Langs    []string
}

type bloomfilter.Options struct {
	FalsePositiveRate float64 // Тут только доля ложноположительных ответов
}

type postgres.Options struct { // нужен лишь для построения индексов
	Host   string
	Port   int
	DBName string
	User   string
	Pass   string
}

type s3client.Options struct { // настройки клиента к хранилищу индексов
	Endpoint        string
	AccessKeyID     string
	SecretAccessKey string
	Region          string
	Secure          bool
	RetriesCount    int
	RetryTimeout    int
}

type s3data.Options struct { // собственно хранилище индексов
	Bucket string
	Name   string
}
```
Для корректной работы спеллчекера поле `SiteDB` не нужно - эти настройки применяются 
для построения и обновления индексов по БД сайта.

`S3Client` и `S3Data` описывают источник данных, из которого считываются данные индексов и bloom-фильтра при конструировании сервиса `wordspell`.
Там должны находиться следующие ресурсы: `ru.index`, `en.index`, `trademark.index`, `bloom.dat`.

Поле `Langs` на данный момент избыточно - там по умолчанию используются два языка - `ru` и `en`.
Это поле предусмотрено на будущее, на данный момент работа корректора опирается на автоматическое распознавание
языка по одному слову. А это распознавание реализовано для трех "языков" - русского, английского и "численного".

Существуют библиотеки, которые довольно достоверно позволяют распознавать больше языков, но здесь они пока не используются.

Пример использования сервиса - вот тут: [examples/speller/main.go](./examples/speller/main.go). 
Здесь `SiteDB` используется лишь как место хранения результатов для оценки, 
а исходные данные берутся из файла, содержащего реальные поисковые запросы.

### Как `wordspell` определяет язык и для чего это нужно?

Язык определяется для слова, а не для всего запроса.

* Если слово содержит только цифры и (возможно) одну точку или запятую, это число. Спеллер ничего не делает с таким словом - просто возвращает "как есть".
* Для русского языка в слове имеют право находиться русские буквы и дефис.
* Для английского языка - английские буквы, дефис, `backtick` и одинарная кавычка.

Если слово - не число, мы подсчитываем в нем количество валидных и невалидных символов. 
Если валидных символов больше, а невалидных - не более 2, то мы относим слово к языку. 
Так что, в принципе, мы можем передать в `wordspell` слово с двумя пробелами, 
и если в нем окажется больше двух русских букв и не окажется ничего больше, 
это будет русское слово. Если в слове два невалидных символа, в каком-то из удалений не останется ни одного, 
и исправление для такого слова может быть найдено.

`wordspell` принимает поисковый запрос целиком, и сам очищает его от лишних символов и разбивает на слова. 
* Торговые марки ищутся по полному соответствию, включая регистр ("Sony" не равно "sony") и без учета языка 
(компания "Пупкин Ltd." будет найдена, если только присутствует в индексе торговых марок, 
и оставлена без изменения точно там же, где и была в изначальном запросе).
* Все остальные слова будет приведены к нижнему регистру и исправлены в соответствие с основным алгоритмом, с учетом языка.

### Выбор лучшего исправления

Построение набора всевозможных вставок в удаления - процесс дорогой. 
Поэтому мы возвращаем результат по первому из удалений, для которого его удается найти. 
Но если в рамках одного удаления нашлось несколько возможных исправлений, мы их ранжируем.

* Сначала по количеству вставок - если удалось что-то найти в пределах единичной вставки, возвращаем лучшее по частоте значение, и двойных вставок уже не делаем.
* Потом по частотам - если в рамках удаления нашлось несколько вариантов, возвращаем то из них, с которым связана наибольшая частота.

Получается, мы проверяем не все возможные варианты, так что точность алгоритма, с одной стороны, ожидается несколько ниже возможной, но, поскольку мы проверяем по индексу не удаления, 
а оригинальные (только "восстановленные") слова, эта неточность будет частично скомпенсирована.

А что, если слова нет в словаре, и ему не удается найти исправления? Ну или если ему не удается определить язык
(например, `прямоwalking`)? В этом случае слово будет возвращено "как есть".

### Откуда берутся данные, которые wordspell подгружает на старте?
Для всех индексов существует еще по одному классу - билдеры. `index.Builder` и `trademarkindex.Builder`.
Они используют источники и хранилища данных, представленные интерфейсными типами. В нынешней реализации мы используем 
`postgres` в качестве источника и `S3-бакет` в качестве хранилища, но недолго реализовать практически любые адаптеры,
имплементировав соответствующие интерфейсы. Например, в этом проекте реализовано еще и файловое хранилище, которое используется в тестах.

Эти билдеры используются в сервисе `wordspell.Builder`, который берет на себя всю работу по генерации индексов, расчету bloom-фильтра
и записи всех необходимых данных в Store. Он использует такой же конфиг, как и спеллер, но поле `SiteDB` в нем является обязательным.

`index.Builder` получает названия товаров, категорий и описания товаров из БД, удаляет из описаний HTML-теги (если находит), 
разбивает весь текст на слова, удаляет из них "лишние" символы, приводит к нижнему регистру, измеряет частоты встречаемости, 
и разделяет на два индекса - русский и английский - определив язык по тому же алгоритму, что и спеллер. Но, как показал опыт, в заполненном
каталоге сайта обязательно встречаются опечатки. Чтобы от них избавиться, мы включаем в индекс лишь русские слова, которые встречаются 
чаще 22 раз и английские - чаще 9. Еще мы не вколючаем в индекс слова длиной менее 2 символов.

Билдер строит всевозможные пары слов, находящихся рядом, и относящихся к одному языку. И включает их тоже в индекс соответствующего языка,
если их частота встречаемости превышает 49. Порог выбран опытным путем, пары в индексе нужны для того, чтобы можно было исправлять слова, 
ошибочно написанные слитно (например, `игрушкидля`).

Он записывает в хранилище файлы(ну или что там хранится) `ru.index` и `en.index`.

После построения этих индексов билдер строит всевозможные удаления по всем словам и парам (для обоих языков), и весь этот гигантский объем
добавляет в bloom-фильтр с 0.01 частотой ошибочно положительных ответов (по умолчанию, можно это дело и изменить). Фильтр сериализуется и
записывается в хранилище под именем `bloom.dat`.

Ну а `trademark.index` строится применением `trademarkindex.Builder`. Этот индекс хранится в мапе с ключами по первому слову трейдмарки,
и построен так, чтобы находить лишь те из них, которые представлены в индексе "как есть" - в том же регистре, с некоторыми "разрешенными"
небуквенными символами и, возможно, из нескольких слов на разных языках.

Если билдер отработал без ошибок, можно сразу запускать спеллчекер с тем же конфигом (только `SiteDB` ему не нужен) - все, 
что ему необходимо для работы, будет записано в хранилище.

Билдер работает недолго с точки зрения стороннего наблюдателя, но достаточно долго с точки зрения сервиса. Поэтому предлагаю гонять
его отдельным кронджобом, а сервис после этого перезапускать путем передеплоя текущей версии.

Пример конструирования и запуска билдера wordspell можно посмотреть здесь: [examples/indexbuilder/main.go](./examples/indexbuilder/main.go).
